# -*- coding: utf-8 -*-
"""Pyramid Vision Transformer V2 - Github.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11S4HWNqR2fW2ZnNJ9r6vnr6zcTztHdNz
"""

import tensorflow as tf

class AdaptivePooling(tf.keras.layers.Layer):
  def __init__(self , outputSize=(7 , 7)):
    super().__init__()
    self.outputSize = outputSize

  def call(self , inputs):
    height , width = tf.shape(inputs)[1] , tf.shape(inputs)[2]

    #(n - f) / f + 1 = o -> (n - f) / f = (o - 1) -> n - f = (o - 1)f -> n = (o - 1)f + f -> n = f(o - 1 + 1) -> n = fo -> f = n / o

    windowSize  = (height // self.outputSize[0] , width // self.outputSize[1])

    return tf.nn.avg_pool2d(inputs , windowSize , strides=windowSize , padding="VALID")

"""#Patch Embedding"""

class PatchEmbedding(tf.keras.layers.Layer):
  def __init__(self , projection_dim , window_size , strides): 
    super().__init__()

    self.projection_dim = projection_dim
    self.window_size = window_size

    self.projection = tf.keras.layers.Conv2D(projection_dim , window_size , strides=strides , padding="SAME")

  def call(self , inputs):
    x = self.projection(inputs)
    return x

"""#Encoder"""

class MultiHeadAttention(tf.keras.layers.Layer):
  def __init__(self , d_model , heads):
    super().__init__()

    self.d_model = d_model
    self.heads = heads
  
  def splitHeads(self , items):
    reshapedItems = tf.reshape(items , shape=(tf.shape(items)[0] , -1 , self.heads , self.d_model//self.heads))
    return tf.transpose(reshapedItems , perm=[0 , 2 , 1 , 3])
  
  def call(self , q , k , v , return_scores=False):
    q = self.splitHeads(q)
    k = self.splitHeads(k)
    v = self.splitHeads(v)

    qk = tf.matmul(q , k , transpose_b=True) / tf.sqrt(tf.cast(tf.shape(q)[-1] , tf.float32))

    attention_scores = tf.nn.softmax(qk , axis=-1)

    output = tf.matmul(attention_scores , v)

    output = tf.reshape(tf.transpose(output , perm=[0 , 2 , 1 , 3]) , shape=(tf.shape(output)[0] , -1 , self.d_model))

    if return_scores:
      return output , attention_scores
    else:
      return output

class LinearMultiHeadAttention(tf.keras.layers.Layer):
  def __init__(self , d_model , heads , poolSize=(7 , 7) , dropout_rate=0.1):
    super().__init__()

    self.wq = tf.keras.layers.Dense(d_model)
    self.wk = tf.keras.layers.Dense(d_model)
    self.wv = tf.keras.layers.Dense(d_model)
    self.wo = tf.keras.layers.Dense(d_model)

    self.pooling = AdaptivePooling(poolSize)

    self.mha = MultiHeadAttention(d_model , heads)

    self.norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)
    self.drop = tf.keras.layers.Dropout(dropout_rate)
  
  def flatten(self , inputs):
    return tf.reshape(inputs , shape=(tf.shape(inputs)[0] , -1 , tf.shape(inputs)[-1]))
  
  def call(self , inputs , return_scores=False , training=True):

    i , j = tf.shape(inputs)[1] , tf.shape(inputs)[2] 

    normed_x = self.norm(inputs)

    x = normed_x 
    c = self.pooling(normed_x)

    q = self.flatten(self.wq(x))
    k = self.flatten(self.wk(c))
    v = self.flatten(self.wv(c))

    output , scores = self.mha(q , k , v , return_scores=True)

    output = tf.reshape(output , shape=(tf.shape(output)[0] , i , j , -1))

    output = self.wo(output)

    output = self.drop(output , training=training)

    output = inputs + output

    if return_scores:
      return output , scores
    else:
      return output

class MLP(tf.keras.layers.Layer):
  def __init__(self , d_model , rate , windowSize=(3 , 3) , dropout=0.1):
    super().__init__()

    self.norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)

    self.model = tf.keras.Sequential([
                                      tf.keras.layers.Dense(d_model * rate),
                                      tf.keras.layers.DepthwiseConv2D(windowSize , padding="SAME" , activation="gelu"),
                                      tf.keras.layers.Dropout(dropout),
                                      tf.keras.layers.Dense(d_model),
                                      tf.keras.layers.Dropout(dropout),
    ])
  
  def call(self , x , training=True):
    normed_x = self.norm(x)
    output = self.model(normed_x , training=training)
    return x + output

class TransformerLayer(tf.keras.layers.Layer):
  def __init__(self , d_model , heads , poolingSize=(7 , 7) , mlp_rate=2 , mlp_windowSize=(3 , 3) , dropout_rate=0.1):
    super().__init__()

    self.mha = LinearMultiHeadAttention(d_model , heads , poolingSize , dropout_rate)
    self.mlp = MLP(d_model , mlp_rate , mlp_windowSize , dropout_rate)

  def call(self , inputs , return_scores=False , training=True):
    out_1 , scores = self.mha(inputs , return_scores=True , training=training)
    out_2 = self.mlp(out_1 , training=training)

    if return_scores:
      return out_2 , scores
    else:
      return out_2

class TransformerEncoder(tf.keras.layers.Layer):
  def __init__(self , d_model , heads , poolingSize=(7 , 7) , mlp_rate=2 , mlp_windowSize=(3 , 3) , layers=1 , dropout_rate=0.1):
    super().__init__()

    self.encoders = [TransformerLayer(d_model , heads , poolingSize , mlp_rate , mlp_windowSize , dropout_rate) for _ in range(layers)]
  
  def call(self , inputs , return_scores=False , training=True):
    x = inputs
    scores = []

    for encoder in self.encoders:
      x , score = encoder(x , return_scores=True , training=training)
      scores.append(score)
    
    if return_scores:
      return x , scores
    else:
      return x

"""#Each PVT V2 Stage"""

class PVTV2Stage(tf.keras.layers.Layer):
  def __init__(self , d_model , heads , windowSize , strides=(2 , 2) , poolingSize=(7 , 7) , mlp_rate=2 , mlp_windowSize=(3 , 3) , layers=1 , dropout_rate=0.1):
    super().__init__()
    
    self.embeddingLayer = PatchEmbedding(d_model , windowSize , strides)
    self.encoders = [TransformerEncoder(d_model , heads , poolingSize , mlp_rate , mlp_windowSize , layers , dropout_rate) for _ in range(layers)]

  def call(self , inputs , return_scores=False , training=True):
    x = self.embeddingLayer(inputs)
    
    scores = []

    for encoder in self.encoders:
      x , score = encoder(x , return_scores=True , training=training)
      scores.append(score)
    
    if return_scores:
      return x , scores
    else:
      return x

"""#PVT V2"""

class PVTV2(tf.keras.Model):
  def __init__(self , num_of_classes , stages , dropout=0.5):
    super().__init__()

    self.num_of_classes = num_of_classes
    self.stages = stages

    self.pooling = tf.keras.layers.GlobalAveragePooling2D()

    self.predictionHead = tf.keras.Sequential([
                                               tf.keras.layers.GlobalAveragePooling2D(),
                                               tf.keras.layers.Dropout(dropout),
                                               tf.keras.layers.Dense(num_of_classes , activation="softmax"),
    ])

  def call(self , inputs , return_scores=False , training=True):
    x = inputs
    scores = []

    for stage in self.stages:
      x , score = stage(x , return_scores=True , training=training)
      scores.append(score)
    
    x = self.predictionHead(x , training=training)

    if return_scores:
      return x , scores
    else:
      return x

"""#Complete Model"""

model = PVTV2(100 , [
                   PVTV2Stage(d_model=64  , windowSize=(2 , 2) , heads=1 , poolingSize=(7 , 7) , mlp_rate=2 , mlp_windowSize=(3 , 3) , layers=2 , dropout_rate=0.1),
                   PVTV2Stage(d_model=128 , windowSize=(2 , 2) , heads=2 , poolingSize=(7 , 7) , mlp_rate=2 , mlp_windowSize=(3 , 3) , layers=2 , dropout_rate=0.1),
                   PVTV2Stage(d_model=320 , windowSize=(2 , 2) , heads=5 , poolingSize=(7 , 7) , mlp_rate=2 , mlp_windowSize=(3 , 3) , layers=2 , dropout_rate=0.1),
])