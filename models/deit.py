# -*- coding: utf-8 -*-
"""DeiT Vision Transformer - Github.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xfvH5Cum63BOiGb6f0_ewBU6Mdx3xrks
"""

import tensorflow as tf

class PatchEmbedding(tf.keras.layers.Layer):
  def __init__(self , size , num_of_patches , projection_dim):
    super().__init__()

    self.size = size
    self.num_of_patches = num_of_patches
    self.projection_dim = projection_dim

    self.projection = tf.keras.layers.Dense(projection_dim)

    self.specialTokens = tf.Variable(tf.keras.initializers.GlorotNormal()(shape=(1 , 2 , projection_dim)) , trainable=True)

    self.positionalEmbedding = tf.keras.layers.Embedding(self.num_of_patches , projection_dim)
    
  
  def call(self , inputs):
    patches = tf.image.extract_patches(inputs , sizes=[1 , self.size , self.size , 1] , strides=[1 , self.size , self.size , 1] , rates=[1 , 1 , 1 , 1] ,  padding="VALID")
    patches = tf.reshape(patches , (tf.shape(inputs)[0] , -1 , self.size * self.size * 3))
    patches = self.projection(patches)

    positions = tf.range(0 , self.num_of_patches , 1)[tf.newaxis , ...]
    positionalEmbedding = self.positionalEmbedding(positions)
    patches = patches + positionalEmbedding

    specialTokens = tf.repeat(self.specialTokens , tf.shape(inputs)[0] , 0)
    patches = tf.concat((specialTokens , patches) , axis=1)

    return patches

class TransformerLayer(tf.keras.layers.Layer):
  def __init__(self , d_model , heads , mlp_rate , dropout_rate=0.1):
    super().__init__()

    self.layernorm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)

    self.mha = tf.keras.layers.MultiHeadAttention(heads , d_model//heads , dropout=dropout_rate)

    self.layernorm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
    
    self.mlp = tf.keras.Sequential([
                                    tf.keras.layers.Dense(d_model * mlp_rate , activation="gelu"),
                                    tf.keras.layers.Dropout(dropout_rate),
                                    tf.keras.layers.Dense(d_model , activation="gelu"),
                                    tf.keras.layers.Dropout(dropout_rate)
    ])
  
  def call(self , inputs , training=True):
    out_1 = self.layernorm_1(inputs)
    out_1 = self.mha(out_1 , out_1 , training=training)
    out_1 = inputs + out_1

    out_2 = self.layernorm_2(out_1)
    out_2 = self.mlp(out_2 , training=training)
    out_2 = out_1 + out_2

    return out_2

class TransformerEncoder(tf.keras.layers.Layer):
  def __init__(self , d_model , heads , mlp_rate , num_layers=1 , dropout_rate=0.1):
    super().__init__()

    self.encoders = [TransformerLayer(d_model , heads , mlp_rate , dropout_rate) for _ in range(num_layers)]
  
  def call(self , inputs , training=True):
    x = inputs

    for layer in self.encoders:
      x = layer(x , training=training)
    
    return x

class DeiTLoss(tf.keras.losses.Loss):
  def __init__(self , temperature=1. , alpha=0.5 , hard=False):
    super().__init__()

    self.temperature = temperature
    self.alpha = alpha
    self.hard = hard
  
  def call(self , y_true , y_pred):
    student_logits = y_pred[0]
    distill_logits = y_pred[1]

    student_labels = y_true[0]
    distill_labels = y_true[1]

    loss = tf.keras.losses.categorical_crossentropy(student_labels , student_logits)

    if not self.hard:
      distill_loss = tf.keras.losses.kl_divergence(
          tf.nn.log_softmax(distill_labels / self.temperature, axis=-1),
          tf.nn.log_softmax(distill_logits / self.temperature, axis=-1),)

      distill_loss *= self.temperature ** 2
    else:
      distill_labels = tf.one_hot(tf.argmax(distill_labels , axis=-1)[: , 0] , distill_labels.shape[-1])
      distill_loss = tf.keras.losses.categorical_crossentropy(distill_labels , distill_logits)

    return (loss * (1 - self.alpha)) + (distill_loss * self.alpha)

class DeiT(tf.keras.Model):
  def __init__(self , num_classes , patch_size , num_of_patches , d_model , heads , num_layers , mlp_rate , teacherModel , temperature=1. , alpha=0.5 , hard=False  , dropout_rate=0.1 , prediction_dropout=0.3):
    super().__init__()

    self.compute_loss = DeiTLoss(temperature , alpha , hard)

    self.teacherModel = teacherModel

    self.teacherModel.trainable = False
    
    self.patchEmbedding = PatchEmbedding(patch_size , num_of_patches , d_model)

    self.encoder = TransformerEncoder(d_model , heads , mlp_rate , num_layers , dropout_rate)

    self.predict_mlp = tf.keras.Sequential([
                                           tf.keras.layers.LayerNormalization(epsilon=1e-6),
                                           tf.keras.layers.Dropout(prediction_dropout),
                                           tf.keras.layers.Dense(mlp_rate * d_model , activation="gelu"),
                                           tf.keras.layers.Dropout(prediction_dropout),
                                           tf.keras.layers.Dense(num_classes)
    ])

    self.distill_mlp = tf.keras.Sequential([
                                           tf.keras.layers.LayerNormalization(epsilon=1e-6),
                                           tf.keras.layers.Dropout(prediction_dropout),
                                           tf.keras.layers.Dense(mlp_rate * d_model , activation="gelu"),
                                           tf.keras.layers.Dropout(prediction_dropout),
                                           tf.keras.layers.Dense(num_classes)
    ])
  
  def call(self , inputs , training=True):
    patches = self.patchEmbedding(inputs)

    encoderResult = self.encoder(patches , training=training)

    clsToken , distillToken = encoderResult[: , 0 , :] , encoderResult[: , 1 , :]

    predict_result = self.predict_mlp(clsToken , training=training)

    distill_result = self.distill_mlp(distillToken , training=training)

    if training:
      return predict_result , distill_result
    else:
      return tf.nn.log_softmax(predict_result , axis=-1)
  
  def train_step(self, data):
        x = data[0]

        y_prediction = data[1]

        y_distillation = self.teacherModel(x , training=False)

        y_true = (y_prediction , y_distillation)

        with tf.GradientTape() as tape:
            y_pred = self(x, training=True)
            loss = self.compute_loss(y_true , y_pred)

        # Compute gradients
        trainable_vars = self.trainable_variables
        gradients = tape.gradient(loss, trainable_vars)
        # Update weights
        self.optimizer.apply_gradients(zip(gradients, trainable_vars))
        # Update metrics (includes the metric that tracks the loss)
        self.compiled_metrics.update_state(y_true[0] , y_pred[0])
        # Return a dict mapping metric names to current value
        return {m.name: m.result() for m in self.metrics}

DeiTClassfier = DeiT(
                    num_classes=1000,
                    patch_size=16,
                    num_of_patches=(224//16)**2,
                    d_model=128,
                    heads=2,
                    num_layers=4,
                    mlp_rate=2,
                    teacherModel=teacherModel,
                    dropout_rate=0.1,
)