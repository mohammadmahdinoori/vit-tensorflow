# -*- coding: utf-8 -*-
"""Convolutional Vision Transformer - Github.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1b0jfaUdraldO_OrBXfQP3lF9TcIq3SS-
"""

import tensorflow as tf
import matplotlib.pyplot as plt

"""#Multi-Head Attention"""

class MultiHeadAttention(tf.keras.layers.Layer):
  def __init__(self , d_model , heads):
    super().__init__()

    self.d_model = d_model
    self.heads = heads
  
  def splitHeads(self , items):
    reshapedItems = tf.reshape(items , shape=(tf.shape(items)[0] , -1 , self.heads , self.d_model//self.heads))
    return tf.transpose(reshapedItems , perm=[0 , 2 , 1 , 3])
  
  def call(self , q , k , v , return_scores=False):
    q = self.splitHeads(q)
    k = self.splitHeads(k)
    v = self.splitHeads(v)

    qk = tf.matmul(q , k , transpose_b=True) / tf.sqrt(tf.cast(tf.shape(q)[-1] , tf.float32))

    attention_scores = tf.nn.softmax(qk , axis=-1)

    output = tf.matmul(attention_scores , v)

    output = tf.reshape(tf.transpose(output , perm=[0 , 2 , 1 , 3]) , shape=(tf.shape(output)[0] , -1 , self.d_model))

    if return_scores:
      return output , attention_scores
    else:
      return output

"""#Convolutional Multi-Head Attention"""

class ConvolutionalMultiHeadAttention(tf.keras.layers.Layer):
  def __init__(self , d_model , heads , projectionWindowSize , projectionStrides , dropoutRate=0.1):
    super().__init__()

    self.mha = MultiHeadAttention(d_model , heads)

    self.wq = tf.keras.layers.SeparableConv2D(d_model , projectionWindowSize , strides=(1 , 1) , padding="SAME")
    self.wk = tf.keras.layers.SeparableConv2D(d_model , projectionWindowSize , strides=projectionStrides , padding="SAME")
    self.wv = tf.keras.layers.SeparableConv2D(d_model , projectionWindowSize , strides=projectionStrides , padding="SAME")

    self.wo = tf.keras.layers.Dense(d_model)

    self.dropout = tf.keras.layers.Dropout(dropoutRate)

    self.norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)
  
  def flatten(self , items):
    return tf.reshape(items , shape=(tf.shape(items)[0] , -1 , tf.shape(items)[-1]))

  def call(self , x , return_scores=False , training=True):
    batchSize = tf.shape(x)[0]

    i = tf.shape(x)[1]
    j = tf.shape(x)[2]

    normed_x = self.norm(x)

    q = self.flatten(self.wq(normed_x))
    k = self.flatten(self.wk(normed_x))
    v = self.flatten(self.wv(normed_x))

    attention_output , attention_scores = self.mha(q , k , v , return_scores=True)

    attention_output = tf.reshape(attention_output , shape=(batchSize , i , j , -1))

    attention_output = self.wo(attention_output)

    output = x + self.dropout(attention_output , training=training)

    if return_scores:
      return output , attention_scores
    else:
      return output

"""#MLP"""

class MLP(tf.keras.layers.Layer):
  def __init__(self , d_model , rate , dropout=0.1):
    super().__init__()

    self.norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)

    self.model = tf.keras.Sequential([
                                      tf.keras.layers.Dense(d_model * rate , activation="gelu"),
                                      tf.keras.layers.Dropout(dropout),
                                      tf.keras.layers.Dense(d_model , activation="gelu"),
                                      tf.keras.layers.Dropout(dropout),
    ])
  
  def call(self , x , training=True):
    normed_x = self.norm(x)
    output = self.model(normed_x , training=training)
    return x + output

"""#Convolutional Transformer"""

class ConvolutionalTransformer(tf.keras.layers.Layer):
  def __init__(self , d_model , heads , projectionWindowSize=(3 , 3) , projectionStrides=(2 , 2) , ffnRate=2 , dropoutRate=0.1):
    super().__init__()

    self.mha = ConvolutionalMultiHeadAttention(d_model , heads , projectionWindowSize , projectionStrides , dropoutRate)
    self.mlp = MLP(d_model , ffnRate , dropoutRate)

  def call(self , x , return_scores=False , training=True):
    att_output , att_scores = self.mha(x , True , training=training)
    output = self.mlp(att_output , training=training)

    if return_scores:
      return output , att_scores
    else:
      return output

"""#Each Stage Of CVT"""

class CvTStage(tf.keras.layers.Layer):
  def __init__(self , projectionDim , heads , embeddingWindowSize , embeddingStrides , layers=1 , projectionWindowSize=(3 , 3) , projectionStrides=(2 , 2) , ffnRate=2 , dropoutRate=0.1):
    super().__init__()
    
    self.embeddingLayer = tf.keras.layers.Conv2D(projectionDim , embeddingWindowSize , strides=embeddingStrides , padding="SAME")
    self.encoders = [ConvolutionalTransformer(projectionDim , heads , projectionWindowSize , projectionStrides , ffnRate , dropoutRate) for _ in range(layers)]

  def call(self , inputs , return_scores=False , training=True):
    x = self.embeddingLayer(inputs)
    
    scores = []

    for encoder in self.encoders:
      x , score = encoder(x , True , training=training)
      scores.append(score)
    
    if return_scores:
      return x , scores
    else:
      return x

"""#CvT"""

class CvT(tf.keras.Model):
  def __init__(self , num_of_classes , stages , dropout=0.5):
    super().__init__()

    self.num_of_classes = num_of_classes
    self.stages = stages

    self.pooling = tf.keras.layers.GlobalAveragePooling2D()

    self.predictionHead = tf.keras.Sequential([
                                               tf.keras.layers.GlobalAveragePooling2D(),
                                               tf.keras.layers.Dropout(dropout),
                                               tf.keras.layers.Dense(num_of_classes , activation="softmax"),
    ])

  def call(self , inputs , return_scores=False , training=True):
    x = inputs
    scores = []

    for stage in self.stages:
      x , score = stage(x , True , training=training)
      scores.append(score)
    
    x = self.predictionHead(x , training=training)

    if return_scores:
      return x , scores
    else:
      return x

"""#Complete Model"""

model = CvT(100 , [
                   CvTStage(projectionDim=64  , heads=1 , embeddingWindowSize=(7 , 7) , embeddingStrides=(4 , 4) , layers=1 , projectionWindowSize=(3 , 3) , projectionStrides=(2 , 2) , ffnRate=4),
                   CvTStage(projectionDim=192 , heads=3 , embeddingWindowSize=(3 , 3) , embeddingStrides=(2 , 2) , layers=1 , projectionWindowSize=(3 , 3) , projectionStrides=(2 , 2) , ffnRate=4),
                   CvTStage(projectionDim=384 , heads=6 , embeddingWindowSize=(3 , 3) , embeddingStrides=(2 , 2) , layers=1 , projectionWindowSize=(3 , 3) , projectionStrides=(2 , 2) , ffnRate=4)
])