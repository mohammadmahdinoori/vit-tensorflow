# -*- coding: utf-8 -*-
"""Pyramid Vision Transformer - Github.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/128jn_vZ6It6vJ-AbwjFJ4onugU4liqf4
"""

import tensorflow as tf

"""#Patch Embedding"""

class PatchEmbedding(tf.keras.layers.Layer):
  def __init__(self , projection_dim , patch_size): 
    super().__init__()

    self.projection_dim = projection_dim
    self.patch_size = patch_size

    self.projection = tf.keras.layers.Conv2D(projection_dim , patch_size , strides=patch_size , padding="VALID")
    self.norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)

  def build(self , input_shape):
    sampleInput = tf.zeros(input_shape)
    output = self.projection(sampleInput)

    self.num_of_patches = output.shape[1] * output.shape[2]
    self.positionalEmbedding = tf.keras.layers.Embedding(self.num_of_patches , self.projection_dim)

  def call(self , inputs , reshape2D=True):
    patches = self.norm(self.projection(inputs))

    i , j  = patches.shape[1] , patches.shape[2]

    patches = tf.reshape(patches , shape=(tf.shape(patches)[0] , -1 , self.projection_dim))
    patches = patches + self.positionalEmbedding(tf.range(0 , self.num_of_patches , 1)[tf.newaxis , ...])

    if reshape2D:
      patches = tf.reshape(patches , shape=(-1 , i , j , self.projection_dim))

    return patches

"""#Encoder"""

class MultiHeadAttention(tf.keras.layers.Layer):
  def __init__(self , d_model , heads):
    super().__init__()

    self.d_model = d_model
    self.heads = heads
  
  def splitHeads(self , items):
    reshapedItems = tf.reshape(items , shape=(tf.shape(items)[0] , -1 , self.heads , self.d_model//self.heads))
    return tf.transpose(reshapedItems , perm=[0 , 2 , 1 , 3])
  
  def call(self , q , k , v , return_scores=False):
    q = self.splitHeads(q)
    k = self.splitHeads(k)
    v = self.splitHeads(v)

    qk = tf.matmul(q , k , transpose_b=True) / tf.sqrt(tf.cast(tf.shape(q)[-1] , tf.float32))

    attention_scores = tf.nn.softmax(qk , axis=-1)

    output = tf.matmul(attention_scores , v)

    output = tf.reshape(tf.transpose(output , perm=[0 , 2 , 1 , 3]) , shape=(tf.shape(output)[0] , -1 , self.d_model))

    if return_scores:
      return output , attention_scores
    else:
      return output

class SpatialReductionAttention(tf.keras.layers.Layer):
  def __init__(self , d_model , heads , reductionFactor=2 , dropout_rate=0.1):
    super().__init__()

    self.wq = tf.keras.layers.Dense(d_model)
    self.wk = tf.keras.layers.Dense(d_model)
    self.wv = tf.keras.layers.Dense(d_model)
    self.wo = tf.keras.layers.Dense(d_model)

    self.sr = tf.keras.Sequential([
                                  tf.keras.layers.Conv2D(d_model , (reductionFactor , reductionFactor) , strides=(reductionFactor , reductionFactor) , padding="VALID"),
                                  tf.keras.layers.LayerNormalization(epsilon=1e-6),
    ])

    self.mha = MultiHeadAttention(d_model , heads)

    self.norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)
    self.drop = tf.keras.layers.Dropout(dropout_rate)
  
  def flatten(self , inputs):
    return tf.reshape(inputs , shape=(tf.shape(inputs)[0] , -1 , tf.shape(inputs)[-1]))
  
  def call(self , inputs , return_scores=False , training=True):

    i , j = tf.shape(inputs)[1] , tf.shape(inputs)[2] 

    normed_x = self.norm(inputs)

    x = normed_x 
    c = self.sr(normed_x)

    q = self.flatten(self.wq(x))
    k = self.flatten(self.wk(c))
    v = self.flatten(self.wv(c))

    output , scores = self.mha(q , k , v , return_scores=True)

    output = tf.reshape(output , shape=(tf.shape(output)[0] , i , j , -1))

    output = self.wo(output)

    output = self.drop(output , training=training)

    output = inputs + output

    if return_scores:
      return output , scores
    else:
      return output

class MLP(tf.keras.layers.Layer):
  def __init__(self , d_model , rate , dropout=0.1):
    super().__init__()

    self.norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)

    self.model = tf.keras.Sequential([
                                      tf.keras.layers.Dense(d_model * rate , activation="gelu"),
                                      tf.keras.layers.Dropout(dropout),
                                      tf.keras.layers.Dense(d_model),
                                      tf.keras.layers.Dropout(dropout),
    ])
  
  def call(self , x , training=True):
    normed_x = self.norm(x)
    output = self.model(normed_x , training=training)
    return x + output

class TransformerLayer(tf.keras.layers.Layer):
  def __init__(self , d_model , heads , reductionFactor=2 , mlp_rate=2 , dropout_rate=0.1):
    super().__init__()

    self.sra = SpatialReductionAttention(d_model , heads , reductionFactor , dropout_rate)
    self.mlp = MLP(d_model , mlp_rate , dropout_rate)

  def call(self , inputs , return_scores=False , training=True):
    out_1 , scores = self.sra(inputs , return_scores=True , training=training)
    out_2 = self.mlp(out_1 , training=training)

    if return_scores:
      return out_2 , scores
    else:
      return out_2

class TransformerEncoder(tf.keras.layers.Layer):
  def __init__(self , d_model , heads , reductionFactor=2 , mlp_rate=2 , layers=1 , dropout_rate=0.1):
    super().__init__()

    self.encoders = [TransformerLayer(d_model , heads , reductionFactor , mlp_rate , dropout_rate) for _ in range(layers)]
  
  def call(self , inputs , return_scores=False , training=True):
    x = inputs
    scores = []

    for encoder in self.encoders:
      x , score = encoder(x , return_scores=True , training=training)
      scores.append(score)
    
    if return_scores:
      return x , scores
    else:
      return x

"""#Each PVT Stage"""

class PVTStage(tf.keras.layers.Layer):
  def __init__(self , d_model , patch_size , heads , reductionFactor=2 , mlp_rate=2 , layers=1 , dropout_rate=0.1):
    super().__init__()
    
    self.embeddingLayer = PatchEmbedding(d_model , patch_size)
    self.encoders = [TransformerEncoder(d_model , heads , reductionFactor , mlp_rate , layers , dropout_rate) for _ in range(layers)]

  def call(self , inputs , return_scores=False , training=True):
    x = self.embeddingLayer(inputs)
    
    scores = []

    for encoder in self.encoders:
      x , score = encoder(x , return_scores=True , training=training)
      scores.append(score)
    
    if return_scores:
      return x , scores
    else:
      return x

"""#PVT"""

class PVT(tf.keras.Model):
  def __init__(self , num_of_classes , stages , dropout=0.5):
    super().__init__()

    self.num_of_classes = num_of_classes
    self.stages = stages

    self.pooling = tf.keras.layers.GlobalAveragePooling2D()

    self.predictionHead = tf.keras.Sequential([
                                               tf.keras.layers.GlobalAveragePooling2D(),
                                               tf.keras.layers.Dropout(dropout),
                                               tf.keras.layers.Dense(num_of_classes , activation="softmax"),
    ])

  def call(self , inputs , return_scores=False , training=True):
    x = inputs
    scores = []

    for stage in self.stages:
      x , score = stage(x , return_scores=True , training=training)
      scores.append(score)
    
    x = self.predictionHead(x , training=training)

    if return_scores:
      return x , scores
    else:
      return x

"""#Complete Model"""

model = PVT(100 , [
                   PVTStage(d_model=64  , patch_size=(2 , 2) , heads=1 , reductionFactor=2 , mlp_rate=2 , layers=2 , dropout_rate=0.1),
                   PVTStage(d_model=128 , patch_size=(2 , 2) , heads=2 , reductionFactor=2 , mlp_rate=2 , layers=2 , dropout_rate=0.1),
                   PVTStage(d_model=320 , patch_size=(2 , 2) , heads=5 , reductionFactor=2 , mlp_rate=2 , layers=2 , dropout_rate=0.1),
])